{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd835347-f5aa-4db9-a976-b5c1cc082c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME = /gz-data/jdk/current/lib\n",
      "JVM_PATH  = /gz-data/jdk/current/lib/server/libjvm.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12224/42501869.py:39: DeprecationWarning: Call to deprecated function (or staticmethod) started. (use pt.java.started() instead) -- Deprecated since version 0.11.0.\n",
      "  if not pt.started():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTerrier started = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Picked up JAVA_TOOL_OPTIONS: -Djava.io.tmpdir=/gz-data/tmp\n",
      "Picked up _JAVA_OPTIONS: -Xms512m -Xmx8g\n",
      "Java started and loaded: pyterrier.java, pyterrier.terrier.java [version=5.11 (build: craig.macdonald 2025-01-13 21:29), helper_version=0.0.8]\n",
      "/tmp/ipykernel_12224/42501869.py:41: DeprecationWarning: Call to deprecated method pt.init(). Deprecated since version 0.11.0.\n",
      "java is now started automatically with default settings. To force initialisation early, run:\n",
      "pt.java.init() # optional, forces java initialisation\n",
      "  pt.init()  # 或 pt.init(tail=False)\n",
      "/tmp/ipykernel_12224/42501869.py:42: DeprecationWarning: Call to deprecated function (or staticmethod) started. (use pt.java.started() instead) -- Deprecated since version 0.11.0.\n",
      "  print(\"PyTerrier started =\", pt.started())\n"
     ]
    }
   ],
   "source": [
    "# ===== MUST RUN FIRST CELL (before importing pyterrier / jnius) =====\n",
    "import os, glob\n",
    "\n",
    "# A. Prefer system JDK (if it exists)\n",
    "sys_java_home = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
    "candidates = [\n",
    "    os.path.join(sys_java_home, \"lib\", \"server\", \"libjvm.so\"),\n",
    "    \"/gz-data/jdk/current/lib/server/libjvm.so\",  # If installed to /gz-data/jdk/current as per previous steps\n",
    "]\n",
    "\n",
    "jvm_path = next((p for p in candidates if os.path.isfile(p)), None)\n",
    "\n",
    "if jvm_path is None:\n",
    "    # Fallback: Search in common directories\n",
    "    for base in (\"/usr/lib/jvm\", \"/gz-data/jdk\"):\n",
    "        for p in glob.glob(base + \"/**/lib/server/libjvm.so\", recursive=True):\n",
    "            jvm_path = p\n",
    "            break\n",
    "\n",
    "if not jvm_path:\n",
    "    raise RuntimeError(\"Could not find libjvm.so. Please install JDK 17 (system or /gz-data) and try again.\")\n",
    "\n",
    "java_home = os.path.abspath(os.path.join(jvm_path, \"..\", \"..\"))  # Remove /lib/server\n",
    "os.environ[\"JAVA_HOME\"] = java_home\n",
    "os.environ[\"JVM_PATH\"]  = jvm_path\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = os.path.dirname(jvm_path) + \":\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
    "os.environ[\"PATH\"] = os.path.join(java_home, \"bin\") + \":\" + os.environ.get(\"PATH\", \"\")\n",
    "\n",
    "# Optional: Specify Java temporary directory and memory\n",
    "os.environ.setdefault(\"JAVA_TOOL_OPTIONS\", \"-Djava.io.tmpdir=/gz-data/tmp\")\n",
    "os.environ.setdefault(\"_JAVA_OPTIONS\", \"-Xms512m -Xmx8g\")\n",
    "os.makedirs(\"/gz-data/tmp\", exist_ok=True)\n",
    "\n",
    "print(\"JAVA_HOME =\", os.environ[\"JAVA_HOME\"])\n",
    "print(\"JVM_PATH  =\", os.environ[\"JVM_PATH\"])\n",
    "\n",
    "# ===== Now import PyTerrier and initialize JVM =====\n",
    "import pyterrier as pt\n",
    "if not pt.started():\n",
    "    # Use standard init for stability (includes Java initialization); add mem/jvm_opts if needed\n",
    "    pt.init()  # Or pt.init(tail=False)\n",
    "print(\"PyTerrier started =\", pt.started())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a3dc56e-01fa-40a3-8b02-7f4a224ea38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU_VISIBLE=0 | SHARD 1/2\n",
      ">>> Loading NLQuAD...\n",
      ">>> Building BM25 Index...\n",
      ">>> Loading LLM...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e872941d7f447db6e8995b1979d2e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Model ready!\n",
      ">>> Shard 1/2 groups = 117 | Subpart 1/4 = 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/29 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  3%|▎         | 1/29 [01:50<51:32, 110.43s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  7%|▋         | 2/29 [12:45<3:13:52, 430.82s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 10%|█         | 3/29 [14:26<2:01:21, 280.06s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 14%|█▍        | 4/29 [17:35<1:41:42, 244.09s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 17%|█▋        | 5/29 [19:30<1:19:00, 197.51s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 21%|██        | 6/29 [22:44<1:15:15, 196.33s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 31%|███       | 9/29 [24:26<33:23, 100.18s/it]  Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 34%|███▍      | 10/29 [26:13<32:12, 101.72s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 38%|███▊      | 11/29 [28:08<31:28, 104.90s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 41%|████▏     | 12/29 [34:15<48:35, 171.49s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 59%|█████▊    | 17/29 [36:10<12:09, 60.83s/it] Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 69%|██████▉   | 20/29 [39:15<09:23, 62.61s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 93%|█████████▎| 27/29 [42:37<00:58, 29.00s/it]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 29/29 [44:03<00:00, 91.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done. Saved:\n",
      "   - /gz-data/nlquad_colbert/final_results_stage1_bm25only_shard1of2_part1of4.csv\n",
      "   - /gz-data/nlquad_colbert/average_metrics_stage1_bm25only_shard1of2_part1of4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ========== Environment & Pipeline Setup (BM25-only + Dual-GPU Sharding) ==========\n",
    "import os, re, torch, random, warnings, pandas as pd\n",
    "from datasets import load_dataset\n",
    "import pyterrier as pt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===== Manual Settings: Single GPU in this Notebook + Sharding Parameters =====\n",
    "GPU_VISIBLE = \"0\"   # Notebook A: \"0\", Notebook B: \"1\"\n",
    "SHARD       = 1     # A=0, B=1\n",
    "NUM_SHARDS  = 2     # Set to 2 for two GPUs in parallel\n",
    "\n",
    "# ====== Cache & Path Configuration ======\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = GPU_VISIBLE\n",
    "os.environ[\"HF_HOME\"] = \"/gz-data/hf_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/gz-data/hf_cache\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/gz-data/hf_cache\"\n",
    "os.makedirs(\"/gz-data/hf_cache\", exist_ok=True)\n",
    "\n",
    "ROOT = \"/gz-data/nlquad_colbert\"\n",
    "os.makedirs(ROOT, exist_ok=True)\n",
    "BM25_TOPK = 5\n",
    "MAXLEN, GEN_MAXLEN = 250, 384\n",
    "MIN_SPLIT_LEN, DESIRED_SEG_LEN = 1000, 250\n",
    "GEN_MODEL = \"/gz-data/models/deepseek-llm-7b-chat\"  # Keep consistent with original (local path)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "random.seed(42); torch.manual_seed(42)\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "\n",
    "print(f\"Using GPU_VISIBLE={GPU_VISIBLE} | SHARD {SHARD}/{NUM_SHARDS}\")\n",
    "\n",
    "# ===== Load Data & Preprocess =====\n",
    "print(\">>> Loading NLQuAD...\")\n",
    "dataset = load_dataset(\"LLukas22/NLQuAD\", split=\"test\")\n",
    "records = []\n",
    "for art in dataset:\n",
    "    for para in art[\"paragraphs\"]:\n",
    "        ctx = para[\"context\"]\n",
    "        if len(ctx.split()) >= MIN_SPLIT_LEN:\n",
    "            cid = para[\"qas\"][0][\"id\"].split(\"_\")[0]\n",
    "            for qa in para[\"qas\"]:\n",
    "                if qa[\"answers\"]:\n",
    "                    records.append({\n",
    "                        \"context_id\": cid,\n",
    "                        \"context\": ctx,\n",
    "                        \"question\": qa[\"question\"],\n",
    "                        \"answer\": qa[\"answers\"][0][\"text\"],\n",
    "                        \"qa_id\": qa[\"id\"]\n",
    "                    })\n",
    "df = pd.DataFrame(records)\n",
    "df = df.sort_values([\"context_id\", \"qa_id\"]).reset_index(drop=True)\n",
    "\n",
    "# ===== Paragraph Splitting =====\n",
    "def semantic_split(text, max_words=DESIRED_SEG_LEN):\n",
    "    sents = re.split(r\"(?<=[.!?])\\s+\", text.strip())\n",
    "    buf, out = [], []\n",
    "    for s in sents:\n",
    "        if s.strip():\n",
    "            buf.append(s)\n",
    "            if len(\" \".join(buf).split()) >= max_words:\n",
    "                out.append(\" \".join(buf))\n",
    "                buf = []\n",
    "    if buf: out.append(\" \".join(buf))\n",
    "    return out\n",
    "\n",
    "para_records = []\n",
    "for cid, grp in df.groupby(\"context_id\"):\n",
    "    context = grp[\"context\"].iloc[0]\n",
    "    for i, seg in enumerate(semantic_split(context)):\n",
    "        para_records.append({\"docno\": f\"{cid}_{i}\", \"text\": seg, \"cid\": cid})\n",
    "para_df = pd.DataFrame(para_records)\n",
    "para_df[\"docid\"] = para_df.index.astype(str)\n",
    "docno_to_docid = dict(zip(para_df[\"docno\"], para_df[\"docid\"]))\n",
    "para_text_map = dict(zip(para_df[\"docid\"], para_df[\"text\"]))\n",
    "\n",
    "def clean_query(q):\n",
    "    return re.sub(r\"[^A-Za-z0-9 ]\", \"\", q.strip())\n",
    "\n",
    "# ===== Read Eligible QIDs (S5 Unified Set) =====\n",
    "ELIGIBLE_CSV = f\"{ROOT}/eligible_qids_top5.csv\"\n",
    "eligible = None\n",
    "if os.path.exists(ELIGIBLE_CSV):\n",
    "    try:\n",
    "        eligible = set(pd.read_csv(ELIGIBLE_CSV)[\"qa_id\"].astype(str))\n",
    "        print(f\">>> Loaded eligible S5 set: {len(eligible)} qids\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to load {ELIGIBLE_CSV}: {e}. Falling back to >=5 check.\")\n",
    "\n",
    "# ===== BM25 Index =====\n",
    "print(\">>> Building BM25 Index...\")\n",
    "index_ref = f\"{ROOT}/pt_index\"\n",
    "if not os.path.exists(index_ref):\n",
    "    index_ref = pt.IterDictIndexer(f\"{ROOT}/pt_index\", meta={\"docno\": 44, \"text\": 60000}, overwrite=True).index(para_df.to_dict(\"records\"))\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "\n",
    "# ===== Load LLM (Consistent with Original: 8-bit + device_map=\"auto\" + compile) =====\n",
    "print(\">>> Loading LLM...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(GEN_MODEL, trust_remote_code=True)\n",
    "quant_cfg = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    GEN_MODEL,\n",
    "    quantization_config=quant_cfg,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# Keep original settings (adjust if conflicts arise; unchanged here to meet \"keep other parts identical\" requirement)\n",
    "model = torch.compile(model, mode=\"reduce-overhead\")\n",
    "print(\">>> Model ready!\")\n",
    "\n",
    "# ===== Prompt Template =====\n",
    "def build_prompt(question, context):\n",
    "    return f\"\"\"You are an AI assistant. Based on the context, answer the question in the following format:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Final Answer:\"\"\"\n",
    "\n",
    "# ===== Dynamic Batch Inference =====\n",
    "def batch_generate_dynamic(prompts, initial_bs=8, max_bs=8):\n",
    "    results = []\n",
    "    i, bs = 0, initial_bs\n",
    "    last_safe_bs = initial_bs\n",
    "    while i < len(prompts):\n",
    "        batch = prompts[i:i+bs]\n",
    "        try:\n",
    "            inputs = tokenizer(batch, return_tensors=\"pt\", padding=True,\n",
    "                               truncation=True, max_length=2048).to(DEVICE)\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=GEN_MAXLEN,\n",
    "                min_new_tokens=256,\n",
    "                penalty_alpha=1.2,             # Keep unchanged\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            results.extend([d.strip() for d in decoded])\n",
    "            i += bs\n",
    "            if bs < max_bs:\n",
    "                last_safe_bs = bs\n",
    "                bs = min(bs * 2, max_bs)\n",
    "        except RuntimeError as e:\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                torch.cuda.empty_cache()\n",
    "                print(f\"⚠️ OOM at bs={bs}, rolling back to {last_safe_bs}\")\n",
    "                bs = max(last_safe_bs // 2, 1)\n",
    "                if bs < 1:\n",
    "                    print(\"❌ Even bs=1 failed, aborting.\")\n",
    "                    break\n",
    "            else:\n",
    "                raise\n",
    "    return results\n",
    "\n",
    "# ===== Metric Calculation =====\n",
    "def compute_metrics(gens, refs):\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "    _, _, bert_f1 = bert_score(gens, refs, lang=\"en\", model_type=\"roberta-large\", verbose=False)\n",
    "    res = []\n",
    "    for g, r, b in zip(gens, refs, bert_f1):\n",
    "        scr = scorer.score(r, g)\n",
    "        bleu = sentence_bleu([r.split()], g.split())\n",
    "        res.append({\n",
    "            \"rouge1\": round(scr[\"rouge1\"].fmeasure, 4),\n",
    "            \"rouge2\": round(scr[\"rouge2\"].fmeasure, 4),\n",
    "            \"rougeL\": round(scr[\"rougeL\"].fmeasure, 4),\n",
    "            \"bleu\": round(bleu, 4),\n",
    "            \"bertscore\": round(b.item(), 4)\n",
    "        })\n",
    "    return res\n",
    "\n",
    "# ====== Answer Cleaning ======\n",
    "def clean_answer(text):\n",
    "    text = text.strip()\n",
    "    if \"Final Answer:\" in text:\n",
    "        return text.split(\"Final Answer:\", 1)[1].strip()\n",
    "    paragraphs = re.split(r\"\\n\\s*\\n\", text)\n",
    "    cleaned_paras = []\n",
    "    for para in paragraphs:\n",
    "        first_line = para.strip().splitlines()[0] if para.strip() else \"\"\n",
    "        if first_line.startswith((\"Question:\", \"Context:\", \"RULES:\", \"Answer the question\")):\n",
    "            continue\n",
    "        cleaned_paras.append(para.strip())\n",
    "    return \"\\n\\n\".join(p for p in cleaned_paras if p)\n",
    "\n",
    "# ===== Top-K Configuration =====\n",
    "CTX_TOPK_LIST = [1, 2, 3, 4, 5]\n",
    "topk_list = [(k, f\"top{k}\") for k in CTX_TOPK_LIST]\n",
    "MAX_REQUIRED_K = max(CTX_TOPK_LIST)  # 5\n",
    "\n",
    "# ====== Sharding (Key Addition): Assign context_id to this shard ======\n",
    "# Only modify these lines, keep others unchanged\n",
    "NUM_SUBPARTS = 4         # Fixed: split into half\n",
    "SUBPART = 1              # B=0; A assisting=1\n",
    "\n",
    "all_groups = list(df.groupby(\"context_id\"))\n",
    "shard_groups = [g for i, g in enumerate(all_groups) if i % NUM_SHARDS == SHARD]\n",
    "sub_groups = [g for j, g in enumerate(shard_groups) if j % NUM_SUBPARTS == SUBPART]\n",
    "\n",
    "print(f\">>> Shard {SHARD}/{NUM_SHARDS} groups = {len(shard_groups)} | \"\n",
    "      f\"Subpart {SUBPART}/{NUM_SUBPARTS} = {len(sub_groups)}\")\n",
    "\n",
    "# ===== Main Loop (BM25-only, Remove ColBERT Reranking, Keep Others Unchanged) =====\n",
    "results = []\n",
    "\n",
    "for cid, grp in tqdm(sub_groups, total=len(sub_groups)):\n",
    "    batch_prompts, meta = [], []\n",
    "    for _, row in grp.iterrows():\n",
    "        q, gt, qid = row[\"question\"], row[\"answer\"], str(row[\"qa_id\"])\n",
    "\n",
    "        if eligible is not None and qid not in eligible:\n",
    "            continue\n",
    "\n",
    "        # --- BM25 with cid filtering ---\n",
    "        bm25_in = pd.DataFrame({\"qid\": [\"0\"], \"query\": [clean_query(q)]})\n",
    "        out = bm25.transform(bm25_in)\n",
    "        out = out[out[\"docno\"].str.startswith(cid)].head(BM25_TOPK)\n",
    "        ids = [int(docno_to_docid[d]) for d in out[\"docno\"] if d in docno_to_docid]\n",
    "\n",
    "        # Uniform sample requirement (ensure top5 available)\n",
    "        if eligible is None and len(ids) < MAX_REQUIRED_K:\n",
    "            continue\n",
    "\n",
    "        paras = [para_text_map[str(i)] for i in ids]\n",
    "\n",
    "        # ===== Remove ColBERT reranking, use BM25 order directly =====\n",
    "        pairs = list(zip(ids, paras, [None]*len(paras)))\n",
    "\n",
    "        if len(pairs) < MAX_REQUIRED_K:\n",
    "            continue\n",
    "\n",
    "        # --- Slice BM25 list, record topk \"rank1/2/..\" text ---\n",
    "        for topk, tag in topk_list:\n",
    "            _, top_ps, _ = zip(*pairs[:topk])\n",
    "            topk_ranked_context = \"\\n\".join([f\"rank{i+1}: {top_ps[i]}\" for i in range(len(top_ps))])\n",
    "\n",
    "            numbered = [f\"Paragraph {i+1}: {p}\" for i, p in enumerate(top_ps)]\n",
    "\n",
    "            # Run all topk sequentially; add reversed for topk>=2; add shuffled for topk>=3\n",
    "            strategies = [(\"sequential\", numbered)]\n",
    "            if topk >= 2 and len(numbered) > 1:\n",
    "                strategies += [(\"reversed\", list(reversed(numbered)))]\n",
    "            if topk >= 3 and len(numbered) > 1:\n",
    "                strategies += [(\"shuffled\", random.sample(numbered, len(numbered)))]\n",
    "\n",
    "            for strat_name, context in strategies:\n",
    "                batch_prompts.append(build_prompt(q, \"\\n\".join(context)))\n",
    "                meta.append((f\"{strat_name}_{tag}\", gt, row[\"qa_id\"], q, topk, topk_ranked_context, cid))\n",
    "\n",
    "    if not batch_prompts:\n",
    "        continue\n",
    "\n",
    "    answers = batch_generate_dynamic(batch_prompts, initial_bs=8, max_bs=8)\n",
    "    for ans, (strat, gt, qid, q, topk_val, topk_ranked_ctx, cid_val) in zip(answers, meta):\n",
    "        if not ans:\n",
    "            continue\n",
    "        ans_clean = clean_answer(ans)\n",
    "        m = compute_metrics([ans_clean], [gt])[0]\n",
    "        results.append({\n",
    "            \"cid\": cid_val,\n",
    "            \"qid\": qid,\n",
    "            \"question\": q,\n",
    "            \"topk\": topk_val,\n",
    "            \"topk_ranked_context\": topk_ranked_ctx,  # Keep column name unchanged\n",
    "            \"strategy\": strat,\n",
    "            \"answer_clean\": ans.strip(),\n",
    "            \"answer_for_eval\": ans_clean,\n",
    "            **m\n",
    "        })\n",
    "\n",
    "# ===== Save Results (With Sharding Suffix; Preserve Auto Line Breaks) =====\n",
    "df_res = pd.DataFrame(results)\n",
    "\n",
    "# Fixed column order\n",
    "cols_order = [\n",
    "    \"cid\", \"qid\", \"question\",\n",
    "    \"topk\", \"topk_ranked_context\", \"strategy\",\n",
    "    \"answer_clean\", \"answer_for_eval\",\n",
    "    \"rouge1\", \"rouge2\", \"rougeL\", \"bleu\", \"bertscore\"\n",
    "]\n",
    "df_res = df_res[cols_order]\n",
    "\n",
    "# Convert \\n in topk_ranked_context to actual newlines (auto-wrap in Excel/tables)\n",
    "df_res[\"topk_ranked_context\"] = df_res[\"topk_ranked_context\"].apply(lambda x: x.replace(\"\\n\", \"\\r\\n\"))\n",
    "\n",
    "suf = f\"_shard{SHARD}of{NUM_SHARDS}_part{SUBPART}of{NUM_SUBPARTS}\"\n",
    "df_res.to_csv(f\"{ROOT}/final_results_stage1_bm25only{suf}.csv\", index=False)\n",
    "avg_m = df_res.groupby(\"strategy\")[[\"rouge1\", \"rouge2\", \"rougeL\", \"bleu\", \"bertscore\"]].mean().reset_index()\n",
    "avg_m.to_csv(f\"{ROOT}/average_metrics_stage1_bm25only{suf}.csv\", index=False)\n",
    "\n",
    "print(\"✅ Done. Saved:\")\n",
    "print(\"   -\", f\"{ROOT}/final_results_stage1_bm25only{suf}.csv\")\n",
    "print(\"   -\", f\"{ROOT}/average_metrics_stage1_bm25only{suf}.csv\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ========== Environment & Pipeline Setup (BM25-only + Dual-GPU Sharding) ==========\n",
    "import os, re, torch, random, warnings, pandas as pd\n",
    "from datasets import load_dataset\n",
    "import pyterrier as pt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===== Manual Settings: Single GPU in this Notebook + Sharding Parameters =====\n",
    "GPU_VISIBLE = \"0\"   # Notebook A: \"0\", Notebook B: \"1\"\n",
    "SHARD       = 1     # A=0, B=1\n",
    "NUM_SHARDS  = 2     # Set to 2 for two GPUs in parallel\n",
    "\n",
    "# ====== Cache & Path Configuration ======\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = GPU_VISIBLE\n",
    "os.environ[\"HF_HOME\"] = \"/gz-data/hf_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/gz-data/hf_cache\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/gz-data/hf_cache\"\n",
    "os.makedirs(\"/gz-data/hf_cache\", exist_ok=True)\n",
    "\n",
    "ROOT = \"/gz-data/nlquad_colbert\"\n",
    "os.makedirs(ROOT, exist_ok=True)\n",
    "BM25_TOPK = 5\n",
    "MAXLEN, GEN_MAXLEN = 250, 384\n",
    "MIN_SPLIT_LEN, DESIRED_SEG_LEN = 1000, 250\n",
    "GEN_MODEL = \"/gz-data/models/deepseek-llm-7b-chat\"  # Keep consistent with original (local path)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "random.seed(42); torch.manual_seed(42)\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "\n",
    "print(f\"Using GPU_VISIBLE={GPU_VISIBLE} | SHARD {SHARD}/{NUM_SHARDS}\")\n",
    "\n",
    "# ===== Load Data & Preprocess =====\n",
    "print(\">>> Loading NLQuAD...\")\n",
    "dataset = load_dataset(\"LLukas22/NLQuAD\", split=\"test\")\n",
    "records = []\n",
    "for art in dataset:\n",
    "    for para in art[\"paragraphs\"]:\n",
    "        ctx = para[\"context\"]\n",
    "        if len(ctx.split()) >= MIN_SPLIT_LEN:\n",
    "            cid = para[\"qas\"][0][\"id\"].split(\"_\")[0]\n",
    "            for qa in para[\"qas\"]:\n",
    "                if qa[\"answers\"]:\n",
    "                    records.append({\n",
    "                        \"context_id\": cid,\n",
    "                        \"context\": ctx,\n",
    "                        \"question\": qa[\"question\"],\n",
    "                        \"answer\": qa[\"answers\"][0][\"text\"],\n",
    "                        \"qa_id\": qa[\"id\"]\n",
    "                    })\n",
    "df = pd.DataFrame(records)\n",
    "df = df.sort_values([\"context_id\", \"qa_id\"]).reset_index(drop=True)\n",
    "\n",
    "# ===== Paragraph Splitting =====\n",
    "def semantic_split(text, max_words=DESIRED_SEG_LEN):\n",
    "    sents = re.split(r\"(?<=[.!?])\\s+\", text.strip())\n",
    "    buf, out = [], []\n",
    "    for s in sents:\n",
    "        if s.strip():\n",
    "            buf.append(s)\n",
    "            if len(\" \".join(buf).split()) >= max_words:\n",
    "                out.append(\" \".join(buf))\n",
    "                buf = []\n",
    "    if buf: out.append(\" \".join(buf))\n",
    "    return out\n",
    "\n",
    "para_records = []\n",
    "for cid, grp in df.groupby(\"context_id\"):\n",
    "    context = grp[\"context\"].iloc[0]\n",
    "    for i, seg in enumerate(semantic_split(context)):\n",
    "        para_records.append({\"docno\": f\"{cid}_{i}\", \"text\": seg, \"cid\": cid})\n",
    "para_df = pd.DataFrame(para_records)\n",
    "para_df[\"docid\"] = para_df.index.astype(str)\n",
    "docno_to_docid = dict(zip(para_df[\"docno\"], para_df[\"docid\"]))\n",
    "para_text_map = dict(zip(para_df[\"docid\"], para_df[\"text\"]))\n",
    "\n",
    "def clean_query(q):\n",
    "    return re.sub(r\"[^A-Za-z0-9 ]\", \"\", q.strip())\n",
    "\n",
    "# ===== Read Eligible QIDs (S5 Unified Set) =====\n",
    "ELIGIBLE_CSV = f\"{ROOT}/eligible_qids_top5.csv\"\n",
    "eligible = None\n",
    "if os.path.exists(ELIGIBLE_CSV):\n",
    "    try:\n",
    "        eligible = set(pd.read_csv(ELIGIBLE_CSV)[\"qa_id\"].astype(str))\n",
    "        print(f\">>> Loaded eligible S5 set: {len(eligible)} qids\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to load {ELIGIBLE_CSV}: {e}. Falling back to >=5 check.\")\n",
    "\n",
    "# ===== BM25 Index =====\n",
    "print(\">>> Building BM25 Index...\")\n",
    "index_ref = f\"{ROOT}/pt_index\"\n",
    "if not os.path.exists(index_ref):\n",
    "    index_ref = pt.IterDictIndexer(f\"{ROOT}/pt_index\", meta={\"docno\": 44, \"text\": 60000}, overwrite=True).index(para_df.to_dict(\"records\"))\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "\n",
    "# ===== Load LLM (Consistent with Original: 8-bit + device_map=\"auto\" + compile) =====\n",
    "print(\">>> Loading LLM...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(GEN_MODEL, trust_remote_code=True)\n",
    "quant_cfg = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    GEN_MODEL,\n",
    "    quantization_config=quant_cfg,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# Keep original settings (adjust if conflicts arise; unchanged here to meet \"keep other parts identical\" requirement)\n",
    "model = torch.compile(model, mode=\"reduce-overhead\")\n",
    "print(\">>> Model ready!\")\n",
    "\n",
    "# ===== Prompt Template =====\n",
    "def build_prompt(question, context):\n",
    "    return f\"\"\"You are an AI assistant. Based on the context, answer the question in the following format:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Final Answer:\"\"\"\n",
    "\n",
    "# ===== Dynamic Batch Inference =====\n",
    "def batch_generate_dynamic(prompts, initial_bs=8, max_bs=8):\n",
    "    results = []\n",
    "    i, bs = 0, initial_bs\n",
    "    last_safe_bs = initial_bs\n",
    "    while i < len(prompts):\n",
    "        batch = prompts[i:i+bs]\n",
    "        try:\n",
    "            inputs = tokenizer(batch, return_tensors=\"pt\", padding=True,\n",
    "                               truncation=True, max_length=2048).to(DEVICE)\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=GEN_MAXLEN,\n",
    "                min_new_tokens=256,\n",
    "                penalty_alpha=1.2,             # Keep unchanged\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            results.extend([d.strip() for d in decoded])\n",
    "            i += bs\n",
    "            if bs < max_bs:\n",
    "                last_safe_bs = bs\n",
    "                bs = min(bs * 2, max_bs)\n",
    "        except RuntimeError as e:\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                torch.cuda.empty_cache()\n",
    "                print(f\"⚠️ OOM at bs={bs}, rolling back to {last_safe_bs}\")\n",
    "                bs = max(last_safe_bs // 2, 1)\n",
    "                if bs < 1:\n",
    "                    print(\"❌ Even bs=1 failed, aborting.\")\n",
    "                    break\n",
    "            else:\n",
    "                raise\n",
    "    return results\n",
    "\n",
    "# ===== Metric Calculation =====\n",
    "def compute_metrics(gens, refs):\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "    _, _, bert_f1 = bert_score(gens, refs, lang=\"en\", model_type=\"roberta-large\", verbose=False)\n",
    "    res = []\n",
    "    for g, r, b in zip(gens, refs, bert_f1):\n",
    "        scr = scorer.score(r, g)\n",
    "        bleu = sentence_bleu([r.split()], g.split())\n",
    "        res.append({\n",
    "            \"rouge1\": round(scr[\"rouge1\"].fmeasure, 4),\n",
    "            \"rouge2\": round(scr[\"rouge2\"].fmeasure, 4),\n",
    "            \"rougeL\": round(scr[\"rougeL\"].fmeasure, 4),\n",
    "            \"bleu\": round(bleu, 4),\n",
    "            \"bertscore\": round(b.item(), 4)\n",
    "        })\n",
    "    return res\n",
    "\n",
    "# ====== Answer Cleaning ======\n",
    "def clean_answer(text):\n",
    "    text = text.strip()\n",
    "    if \"Final Answer:\" in text:\n",
    "        return text.split(\"Final Answer:\", 1)[1].strip()\n",
    "    paragraphs = re.split(r\"\\n\\s*\\n\", text)\n",
    "    cleaned_paras = []\n",
    "    for para in paragraphs:\n",
    "        first_line = para.strip().splitlines()[0] if para.strip() else \"\"\n",
    "        if first_line.startswith((\"Question:\", \"Context:\", \"RULES:\", \"Answer the question\")):\n",
    "            continue\n",
    "        cleaned_paras.append(para.strip())\n",
    "    return \"\\n\\n\".join(p for p in cleaned_paras if p)\n",
    "\n",
    "# ===== Top-K Configuration =====\n",
    "CTX_TOPK_LIST = [1, 2, 3, 4, 5]\n",
    "topk_list = [(k, f\"top{k}\") for k in CTX_TOPK_LIST]\n",
    "MAX_REQUIRED_K = max(CTX_TOPK_LIST)  # 5\n",
    "\n",
    "# ====== Sharding (Key Addition): Assign context_id to this shard ======\n",
    "# Only modify these lines, keep others unchanged\n",
    "NUM_SUBPARTS = 4         # Fixed: split into half\n",
    "SUBPART = 0              # B=0; A assisting=1\n",
    "\n",
    "all_groups = list(df.groupby(\"context_id\"))\n",
    "shard_groups = [g for i, g in enumerate(all_groups) if i % NUM_SHARDS == SHARD]\n",
    "sub_groups = [g for j, g in enumerate(shard_groups) if j % NUM_SUBPARTS == SUBPART]\n",
    "\n",
    "print(f\">>> Shard {SHARD}/{NUM_SHARDS} groups = {len(shard_groups)} | \"\n",
    "      f\"Subpart {SUBPART}/{NUM_SUBPARTS} = {len(sub_groups)}\")\n",
    "\n",
    "# ===== Main Loop (BM25-only, Remove ColBERT Reranking, Keep Others Unchanged) =====\n",
    "results = []\n",
    "\n",
    "for cid, grp in tqdm(sub_groups, total=len(sub_groups)):\n",
    "    batch_prompts, meta = [], []\n",
    "    for _, row in grp.iterrows():\n",
    "        q, gt, qid = row[\"question\"], row[\"answer\"], str(row[\"qa_id\"])\n",
    "\n",
    "        if eligible is not None and qid not in eligible:\n",
    "            continue\n",
    "\n",
    "        # --- BM25 with cid filtering ---\n",
    "        bm25_in = pd.DataFrame({\"qid\": [\"0\"], \"query\": [clean_query(q)]})\n",
    "        out = bm25.transform(bm25_in)\n",
    "        out = out[out[\"docno\"].str.startswith(cid)].head(BM25_TOPK)\n",
    "        ids = [int(docno_to_docid[d]) for d in out[\"docno\"] if d in docno_to_docid]\n",
    "\n",
    "        # Uniform sample requirement (ensure top5 available)\n",
    "        if eligible is None and len(ids) < MAX_REQUIRED_K:\n",
    "            continue\n",
    "\n",
    "        paras = [para_text_map[str(i)] for i in ids]\n",
    "\n",
    "        # ===== Remove ColBERT reranking, use BM25 order directly =====\n",
    "        pairs = list(zip(ids, paras, [None]*len(paras)))\n",
    "\n",
    "        if len(pairs) < MAX_REQUIRED_K:\n",
    "            continue\n",
    "\n",
    "        # --- Slice BM25 list, record topk \"rank1/2/..\" text ---\n",
    "        for topk, tag in topk_list:\n",
    "            _, top_ps, _ = zip(*pairs[:topk])\n",
    "            topk_ranked_context = \"\\n\".join([f\"rank{i+1}: {top_ps[i]}\" for i in range(len(top_ps))])\n",
    "\n",
    "            numbered = [f\"Paragraph {i+1}: {p}\" for i, p in enumerate(top_ps)]\n",
    "\n",
    "            # Run all topk sequentially; add reversed for topk>=2; add shuffled for topk>=3\n",
    "            strategies = [(\"sequential\", numbered)]\n",
    "            if topk >= 2 and len(numbered) > 1:\n",
    "                strategies += [(\"reversed\", list(reversed(numbered)))]\n",
    "            if topk >= 3 and len(numbered) > 1:\n",
    "                strategies += [(\"shuffled\", random.sample(numbered, len(numbered)))]\n",
    "\n",
    "            for strat_name, context in strategies:\n",
    "                batch_prompts.append(build_prompt(q, \"\\n\".join(context)))\n",
    "                meta.append((f\"{strat_name}_{tag}\", gt, row[\"qa_id\"], q, topk, topk_ranked_context, cid))\n",
    "\n",
    "    if not batch_prompts:\n",
    "        continue\n",
    "\n",
    "    answers = batch_generate_dynamic(batch_prompts, initial_bs=8, max_bs=8)\n",
    "    for ans, (strat, gt, qid, q, topk_val, topk_ranked_ctx, cid_val) in zip(answers, meta):\n",
    "        if not ans:\n",
    "            continue\n",
    "        ans_clean = clean_answer(ans)\n",
    "        m = compute_metrics([ans_clean], [gt])[0]\n",
    "        results.append({\n",
    "            \"cid\": cid_val,\n",
    "            \"qid\": qid,\n",
    "            \"question\": q,\n",
    "            \"topk\": topk_val,\n",
    "            \"topk_ranked_context\": topk_ranked_ctx,  # Keep column name unchanged\n",
    "            \"strategy\": strat,\n",
    "            \"answer_clean\": ans.strip(),\n",
    "            \"answer_for_eval\": ans_clean,\n",
    "            **m\n",
    "        })\n",
    "\n",
    "# ===== Save Results (With Sharding Suffix; Preserve Auto Line Breaks) =====\n",
    "df_res = pd.DataFrame(results)\n",
    "\n",
    "# Fixed column order\n",
    "cols_order = [\n",
    "    \"cid\", \"qid\", \"question\",\n",
    "    \"topk\", \"topk_ranked_context\", \"strategy\",\n",
    "    \"answer_clean\", \"answer_for_eval\",\n",
    "    \"rouge1\", \"rouge2\", \"rougeL\", \"bleu\", \"bertscore\"\n",
    "]\n",
    "df_res = df_res[cols_order]\n",
    "\n",
    "# Convert \\n in topk_ranked_context to actual newlines (auto-wrap in Excel/tables)\n",
    "df_res[\"topk_ranked_context\"] = df_res[\"topk_ranked_context\"].apply(lambda x: x.replace(\"\\n\", \"\\r\\n\"))\n",
    "\n",
    "suf = f\"_shard{SHARD}of{NUM_SHARDS}_part{SUBPART}of{NUM_SUBPARTS}\"\n",
    "df_res.to_csv(f\"{ROOT}/final_results_stage1_bm25only{suf}.csv\", index=False)\n",
    "avg_m = df_res.groupby(\"strategy\")[[\"rouge1\", \"rouge2\", \"rougeL\", \"bleu\", \"bertscore\"]].mean().reset_index()\n",
    "avg_m.to_csv(f\"{ROOT}/average_metrics_stage1_bm25only{suf}.csv\", index=False)\n",
    "\n",
    "print(\"✅ Done. Saved:\")\n",
    "print(\"   -\", f\"{ROOT}/final_results_stage1_bm25only{suf}.csv\")\n",
    "print(\"   -\", f\"{ROOT}/average_metrics_stage1_bm25only{suf}.csv\")"
   ],
   "id": "114ca466d5055543"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ========== Environment & Pipeline Setup (BM25-only + Dual-GPU Sharding) ==========\n",
    "import os, re, torch, random, warnings, pandas as pd\n",
    "from datasets import load_dataset\n",
    "import pyterrier as pt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===== Manual Settings: Single GPU in this Notebook + Sharding Parameters =====\n",
    "GPU_VISIBLE = \"1\"   # Notebook A: \"0\", Notebook B: \"1\"\n",
    "SHARD       = 1     # A=0, B=1\n",
    "NUM_SHARDS  = 2     # Set to 2 for two GPUs in parallel\n",
    "\n",
    "# ====== Cache & Path Configuration ======\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = GPU_VISIBLE\n",
    "os.environ[\"HF_HOME\"] = \"/gz-data/hf_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/gz-data/hf_cache\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/gz-data/hf_cache\"\n",
    "os.makedirs(\"/gz-data/hf_cache\", exist_ok=True)\n",
    "\n",
    "ROOT = \"/gz-data/nlquad_colbert\"\n",
    "os.makedirs(ROOT, exist_ok=True)\n",
    "BM25_TOPK = 5\n",
    "MAXLEN, GEN_MAXLEN = 250, 384\n",
    "MIN_SPLIT_LEN, DESIRED_SEG_LEN = 1000, 250\n",
    "GEN_MODEL = \"/gz-data/models/deepseek-llm-7b-chat\"  # Keep consistent with original (local path)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "random.seed(42); torch.manual_seed(42)\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "\n",
    "print(f\"Using GPU_VISIBLE={GPU_VISIBLE} | SHARD {SHARD}/{NUM_SHARDS}\")\n",
    "\n",
    "# ===== Load Data & Preprocess =====\n",
    "print(\">>> Loading NLQuAD...\")\n",
    "dataset = load_dataset(\"LLukas22/NLQuAD\", split=\"test\")\n",
    "records = []\n",
    "for art in dataset:\n",
    "    for para in art[\"paragraphs\"]:\n",
    "        ctx = para[\"context\"]\n",
    "        if len(ctx.split()) >= MIN_SPLIT_LEN:\n",
    "            cid = para[\"qas\"][0][\"id\"].split(\"_\")[0]\n",
    "            for qa in para[\"qas\"]:\n",
    "                if qa[\"answers\"]:\n",
    "                    records.append({\n",
    "                        \"context_id\": cid,\n",
    "                        \"context\": ctx,\n",
    "                        \"question\": qa[\"question\"],\n",
    "                        \"answer\": qa[\"answers\"][0][\"text\"],\n",
    "                        \"qa_id\": qa[\"id\"]\n",
    "                    })\n",
    "df = pd.DataFrame(records)\n",
    "df = df.sort_values([\"context_id\", \"qa_id\"]).reset_index(drop=True)\n",
    "\n",
    "# ===== Paragraph Splitting =====\n",
    "def semantic_split(text, max_words=DESIRED_SEG_LEN):\n",
    "    sents = re.split(r\"(?<=[.!?])\\s+\", text.strip())\n",
    "    buf, out = [], []\n",
    "    for s in sents:\n",
    "        if s.strip():\n",
    "            buf.append(s)\n",
    "            if len(\" \".join(buf).split()) >= max_words:\n",
    "                out.append(\" \".join(buf))\n",
    "                buf = []\n",
    "    if buf: out.append(\" \".join(buf))\n",
    "    return out\n",
    "\n",
    "para_records = []\n",
    "for cid, grp in df.groupby(\"context_id\"):\n",
    "    context = grp[\"context\"].iloc[0]\n",
    "    for i, seg in enumerate(semantic_split(context)):\n",
    "        para_records.append({\"docno\": f\"{cid}_{i}\", \"text\": seg, \"cid\": cid})\n",
    "para_df = pd.DataFrame(para_records)\n",
    "para_df[\"docid\"] = para_df.index.astype(str)\n",
    "docno_to_docid = dict(zip(para_df[\"docno\"], para_df[\"docid\"]))\n",
    "para_text_map = dict(zip(para_df[\"docid\"], para_df[\"text\"]))\n",
    "\n",
    "def clean_query(q):\n",
    "    return re.sub(r\"[^A-Za-z0-9 ]\", \"\", q.strip())\n",
    "\n",
    "# ===== Read Eligible QIDs (S5 Unified Set) =====\n",
    "ELIGIBLE_CSV = f\"{ROOT}/eligible_qids_top5.csv\"\n",
    "eligible = None\n",
    "if os.path.exists(ELIGIBLE_CSV):\n",
    "    try:\n",
    "        eligible = set(pd.read_csv(ELIGIBLE_CSV)[\"qa_id\"].astype(str))\n",
    "        print(f\">>> Loaded eligible S5 set: {len(eligible)} qids\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to load {ELIGIBLE_CSV}: {e}. Falling back to >=5 check.\")\n",
    "\n",
    "# ===== BM25 Index =====\n",
    "print(\">>> Building BM25 Index...\")\n",
    "index_ref = f\"{ROOT}/pt_index\"\n",
    "if not os.path.exists(index_ref):\n",
    "    index_ref = pt.IterDictIndexer(f\"{ROOT}/pt_index\", meta={\"docno\": 44, \"text\": 60000}, overwrite=True).index(para_df.to_dict(\"records\"))\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "\n",
    "# ===== Load LLM (Consistent with Original: 8-bit + device_map=\"auto\" + compile) =====\n",
    "print(\">>> Loading LLM...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(GEN_MODEL, trust_remote_code=True)\n",
    "quant_cfg = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    GEN_MODEL,\n",
    "    quantization_config=quant_cfg,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# Keep original settings (adjust if conflicts arise; unchanged here to meet \"keep other parts identical\" requirement)\n",
    "model = torch.compile(model, mode=\"reduce-overhead\")\n",
    "print(\">>> Model ready!\")\n",
    "\n",
    "# ===== Prompt Template =====\n",
    "def build_prompt(question, context):\n",
    "    return f\"\"\"You are an AI assistant. Based on the context, answer the question in the following format:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Final Answer:\"\"\"\n",
    "\n",
    "# ===== Dynamic Batch Inference =====\n",
    "def batch_generate_dynamic(prompts, initial_bs=8, max_bs=8):\n",
    "    results = []\n",
    "    i, bs = 0, initial_bs\n",
    "    last_safe_bs = initial_bs\n",
    "    while i < len(prompts):\n",
    "        batch = prompts[i:i+bs]\n",
    "        try:\n",
    "            inputs = tokenizer(batch, return_tensors=\"pt\", padding=True,\n",
    "                               truncation=True, max_length=2048).to(DEVICE)\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=GEN_MAXLEN,\n",
    "                min_new_tokens=256,\n",
    "                penalty_alpha=1.2,             # Keep unchanged\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            results.extend([d.strip() for d in decoded])\n",
    "            i += bs\n",
    "            if bs < max_bs:\n",
    "                last_safe_bs = bs\n",
    "                bs = min(bs * 2, max_bs)\n",
    "        except RuntimeError as e:\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                torch.cuda.empty_cache()\n",
    "                print(f\"⚠️ OOM at bs={bs}, rolling back to {last_safe_bs}\")\n",
    "                bs = max(last_safe_bs // 2, 1)\n",
    "                if bs < 1:\n",
    "                    print(\"❌ Even bs=1 failed, aborting.\")\n",
    "                    break\n",
    "            else:\n",
    "                raise\n",
    "    return results\n",
    "\n",
    "# ===== Metric Calculation =====\n",
    "def compute_metrics(gens, refs):\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "    _, _, bert_f1 = bert_score(gens, refs, lang=\"en\", model_type=\"roberta-large\", verbose=False)\n",
    "    res = []\n",
    "    for g, r, b in zip(gens, refs, bert_f1):\n",
    "        scr = scorer.score(r, g)\n",
    "        bleu = sentence_bleu([r.split()], g.split())\n",
    "        res.append({\n",
    "            \"rouge1\": round(scr[\"rouge1\"].fmeasure, 4),\n",
    "            \"rouge2\": round(scr[\"rouge2\"].fmeasure, 4),\n",
    "            \"rougeL\": round(scr[\"rougeL\"].fmeasure, 4),\n",
    "            \"bleu\": round(bleu, 4),\n",
    "            \"bertscore\": round(b.item(), 4)\n",
    "        })\n",
    "    return res\n",
    "\n",
    "# ====== Answer Cleaning ======\n",
    "def clean_answer(text):\n",
    "    text = text.strip()\n",
    "    if \"Final Answer:\" in text:\n",
    "        return text.split(\"Final Answer:\", 1)[1].strip()\n",
    "    paragraphs = re.split(r\"\\n\\s*\\n\", text)\n",
    "    cleaned_paras = []\n",
    "    for para in paragraphs:\n",
    "        first_line = para.strip().splitlines()[0] if para.strip() else \"\"\n",
    "        if first_line.startswith((\"Question:\", \"Context:\", \"RULES:\", \"Answer the question\")):\n",
    "            continue\n",
    "        cleaned_paras.append(para.strip())\n",
    "    return \"\\n\\n\".join(p for p in cleaned_paras if p)\n",
    "\n",
    "# ===== Top-K Configuration =====\n",
    "CTX_TOPK_LIST = [1, 2, 3, 4, 5]\n",
    "topk_list = [(k, f\"top{k}\") for k in CTX_TOPK_LIST]\n",
    "MAX_REQUIRED_K = max(CTX_TOPK_LIST)  # 5\n",
    "\n",
    "# ====== Sharding (Key Addition): Assign context_id to this shard ======\n",
    "# Only modify these lines, keep others unchanged\n",
    "NUM_SUBPARTS = 4         # Fixed: split into half\n",
    "SUBPART = 3              # B=0; A assisting=1\n",
    "\n",
    "all_groups = list(df.groupby(\"context_id\"))\n",
    "shard_groups = [g for i, g in enumerate(all_groups) if i % NUM_SHARDS == SHARD]\n",
    "sub_groups = [g for j, g in enumerate(shard_groups) if j % NUM_SUBPARTS == SUBPART]\n",
    "\n",
    "print(f\">>> Shard {SHARD}/{NUM_SHARDS} groups = {len(shard_groups)} | \"\n",
    "      f\"Subpart {SUBPART}/{NUM_SUBPARTS} = {len(sub_groups)}\")\n",
    "\n",
    "# ===== Main Loop (BM25-only, Remove ColBERT Reranking, Keep Others Unchanged) =====\n",
    "results = []\n",
    "\n",
    "for cid, grp in tqdm(sub_groups, total=len(sub_groups)):\n",
    "    batch_prompts, meta = [], []\n",
    "    for _, row in grp.iterrows():\n",
    "        q, gt, qid = row[\"question\"], row[\"answer\"], str(row[\"qa_id\"])\n",
    "\n",
    "        if eligible is not None and qid not in eligible:\n",
    "            continue\n",
    "\n",
    "        # --- BM25 with cid filtering ---\n",
    "        bm25_in = pd.DataFrame({\"qid\": [\"0\"], \"query\": [clean_query(q)]})\n",
    "        out = bm25.transform(bm25_in)\n",
    "        out = out[out[\"docno\"].str.startswith(cid)].head(BM25_TOPK)\n",
    "        ids = [int(docno_to_docid[d]) for d in out[\"docno\"] if d in docno_to_docid]\n",
    "\n",
    "        # Uniform sample requirement (ensure top5 available)\n",
    "        if eligible is None and len(ids) < MAX_REQUIRED_K:\n",
    "            continue\n",
    "\n",
    "        paras = [para_text_map[str(i)] for i in ids]\n",
    "\n",
    "        # ===== Remove ColBERT reranking, use BM25 order directly =====\n",
    "        pairs = list(zip(ids, paras, [None]*len(paras)))\n",
    "\n",
    "        if len(pairs) < MAX_REQUIRED_K:\n",
    "            continue\n",
    "\n",
    "        # --- Slice BM25 list, record topk \"rank1/2/..\" text ---\n",
    "        for topk, tag in topk_list:\n",
    "            _, top_ps, _ = zip(*pairs[:topk])\n",
    "            topk_ranked_context = \"\\n\".join([f\"rank{i+1}: {top_ps[i]}\" for i in range(len(top_ps))])\n",
    "\n",
    "            numbered = [f\"Paragraph {i+1}: {p}\" for i, p in enumerate(top_ps)]\n",
    "\n",
    "            # Run all topk sequentially; add reversed for topk>=2; add shuffled for topk>=3\n",
    "            strategies = [(\"sequential\", numbered)]\n",
    "            if topk >= 2 and len(numbered) > 1:\n",
    "                strategies += [(\"reversed\", list(reversed(numbered)))]\n",
    "            if topk >= 3 and len(numbered) > 1:\n",
    "                strategies += [(\"shuffled\", random.sample(numbered, len(numbered)))]\n",
    "\n",
    "            for strat_name, context in strategies:\n",
    "                batch_prompts.append(build_prompt(q, \"\\n\".join(context)))\n",
    "                meta.append((f\"{strat_name}_{tag}\", gt, row[\"qa_id\"], q, topk, topk_ranked_context, cid))\n",
    "\n",
    "    if not batch_prompts:\n",
    "        continue\n",
    "\n",
    "    answers = batch_generate_dynamic(batch_prompts, initial_bs=8, max_bs=8)\n",
    "    for ans, (strat, gt, qid, q, topk_val, topk_ranked_ctx, cid_val) in zip(answers, meta):\n",
    "        if not ans:\n",
    "            continue\n",
    "        ans_clean = clean_answer(ans)\n",
    "        m = compute_metrics([ans_clean], [gt])[0]\n",
    "        results.append({\n",
    "            \"cid\": cid_val,\n",
    "            \"qid\": qid,\n",
    "            \"question\": q,\n",
    "            \"topk\": topk_val,\n",
    "            \"topk_ranked_context\": topk_ranked_ctx,  # Keep column name unchanged\n",
    "            \"strategy\": strat,\n",
    "            \"answer_clean\": ans.strip(),\n",
    "            \"answer_for_eval\": ans_clean,\n",
    "            **m\n",
    "        })\n",
    "\n",
    "# ===== Save Results (With Sharding Suffix; Preserve Auto Line Breaks) =====\n",
    "df_res = pd.DataFrame(results)\n",
    "\n",
    "# Fixed column order\n",
    "cols_order = [\n",
    "    \"cid\", \"qid\", \"question\",\n",
    "    \"topk\", \"topk_ranked_context\", \"strategy\",\n",
    "    \"answer_clean\", \"answer_for_eval\",\n",
    "    \"rouge1\", \"rouge2\", \"rougeL\", \"bleu\", \"bertscore\"\n",
    "]\n",
    "df_res = df_res[cols_order]\n",
    "\n",
    "# Convert \\n in topk_ranked_context to actual newlines (auto-wrap in Excel/tables)\n",
    "df_res[\"topk_ranked_context\"] = df_res[\"topk_ranked_context\"].apply(lambda x: x.replace(\"\\n\", \"\\r\\n\"))\n",
    "\n",
    "suf = f\"_shard{SHARD}of{NUM_SHARDS}_part{SUBPART}of{NUM_SUBPARTS}\"\n",
    "df_res.to_csv(f\"{ROOT}/final_results_stage1_bm25only{suf}.csv\", index=False)\n",
    "avg_m = df_res.groupby(\"strategy\")[[\"rouge1\", \"rouge2\", \"rougeL\", \"bleu\", \"bertscore\"]].mean().reset_index()\n",
    "avg_m.to_csv(f\"{ROOT}/average_metrics_stage1_bm25only{suf}.csv\", index=False)\n",
    "\n",
    "print(\"✅ Done. Saved:\")\n",
    "print(\"   -\", f\"{ROOT}/final_results_stage1_bm25only{suf}.csv\")\n",
    "print(\"   -\", f\"{ROOT}/average_metrics_stage1_bm25only{suf}.csv\")"
   ],
   "id": "e242e82d0a8aeb5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ========== Environment & Pipeline Setup (BM25-only + Dual-GPU Sharding) ==========\n",
    "import os, re, torch, random, warnings, pandas as pd\n",
    "from datasets import load_dataset\n",
    "import pyterrier as pt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ===== Manual Settings: Single GPU in this Notebook + Sharding Parameters =====\n",
    "GPU_VISIBLE = \"1\"   # Notebook A: \"0\", Notebook B: \"1\"\n",
    "SHARD       = 1     # A=0, B=1\n",
    "NUM_SHARDS  = 2     # Set to 2 for two GPUs in parallel\n",
    "\n",
    "# ====== Cache & Path Configuration ======\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = GPU_VISIBLE\n",
    "os.environ[\"HF_HOME\"] = \"/gz-data/hf_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/gz-data/hf_cache\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/gz-data/hf_cache\"\n",
    "os.makedirs(\"/gz-data/hf_cache\", exist_ok=True)\n",
    "\n",
    "ROOT = \"/gz-data/nlquad_colbert\"\n",
    "os.makedirs(ROOT, exist_ok=True)\n",
    "BM25_TOPK = 5\n",
    "MAXLEN, GEN_MAXLEN = 250, 384\n",
    "MIN_SPLIT_LEN, DESIRED_SEG_LEN = 1000, 250\n",
    "GEN_MODEL = \"/gz-data/models/deepseek-llm-7b-chat\"  # Keep consistent with original (local path)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "random.seed(42); torch.manual_seed(42)\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "\n",
    "print(f\"Using GPU_VISIBLE={GPU_VISIBLE} | SHARD {SHARD}/{NUM_SHARDS}\")\n",
    "\n",
    "# ===== Load Data & Preprocess =====\n",
    "print(\">>> Loading NLQuAD...\")\n",
    "dataset = load_dataset(\"LLukas22/NLQuAD\", split=\"test\")\n",
    "records = []\n",
    "for art in dataset:\n",
    "    for para in art[\"paragraphs\"]:\n",
    "        ctx = para[\"context\"]\n",
    "        if len(ctx.split()) >= MIN_SPLIT_LEN:\n",
    "            cid = para[\"qas\"][0][\"id\"].split(\"_\")[0]\n",
    "            for qa in para[\"qas\"]:\n",
    "                if qa[\"answers\"]:\n",
    "                    records.append({\n",
    "                        \"context_id\": cid,\n",
    "                        \"context\": ctx,\n",
    "                        \"question\": qa[\"question\"],\n",
    "                        \"answer\": qa[\"answers\"][0][\"text\"],\n",
    "                        \"qa_id\": qa[\"id\"]\n",
    "                    })\n",
    "df = pd.DataFrame(records)\n",
    "df = df.sort_values([\"context_id\", \"qa_id\"]).reset_index(drop=True)\n",
    "\n",
    "# ===== Paragraph Splitting =====\n",
    "def semantic_split(text, max_words=DESIRED_SEG_LEN):\n",
    "    sents = re.split(r\"(?<=[.!?])\\s+\", text.strip())\n",
    "    buf, out = [], []\n",
    "    for s in sents:\n",
    "        if s.strip():\n",
    "            buf.append(s)\n",
    "            if len(\" \".join(buf).split()) >= max_words:\n",
    "                out.append(\" \".join(buf))\n",
    "                buf = []\n",
    "    if buf: out.append(\" \".join(buf))\n",
    "    return out\n",
    "\n",
    "para_records = []\n",
    "for cid, grp in df.groupby(\"context_id\"):\n",
    "    context = grp[\"context\"].iloc[0]\n",
    "    for i, seg in enumerate(semantic_split(context)):\n",
    "        para_records.append({\"docno\": f\"{cid}_{i}\", \"text\": seg, \"cid\": cid})\n",
    "para_df = pd.DataFrame(para_records)\n",
    "para_df[\"docid\"] = para_df.index.astype(str)\n",
    "docno_to_docid = dict(zip(para_df[\"docno\"], para_df[\"docid\"]))\n",
    "para_text_map = dict(zip(para_df[\"docid\"], para_df[\"text\"]))\n",
    "\n",
    "def clean_query(q):\n",
    "    return re.sub(r\"[^A-Za-z0-9 ]\", \"\", q.strip())\n",
    "\n",
    "# ===== Read Eligible QIDs (S5 Unified Set) =====\n",
    "ELIGIBLE_CSV = f\"{ROOT}/eligible_qids_top5.csv\"\n",
    "eligible = None\n",
    "if os.path.exists(ELIGIBLE_CSV):\n",
    "    try:\n",
    "        eligible = set(pd.read_csv(ELIGIBLE_CSV)[\"qa_id\"].astype(str))\n",
    "        print(f\">>> Loaded eligible S5 set: {len(eligible)} qids\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to load {ELIGIBLE_CSV}: {e}. Falling back to >=5 check.\")\n",
    "\n",
    "# ===== BM25 Index =====\n",
    "print(\">>> Building BM25 Index...\")\n",
    "index_ref = f\"{ROOT}/pt_index\"\n",
    "if not os.path.exists(index_ref):\n",
    "    index_ref = pt.IterDictIndexer(f\"{ROOT}/pt_index\", meta={\"docno\": 44, \"text\": 60000}, overwrite=True).index(para_df.to_dict(\"records\"))\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "\n",
    "# ===== Load LLM (Consistent with Original: 8-bit + device_map=\"auto\" + compile) =====\n",
    "print(\">>> Loading LLM...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(GEN_MODEL, trust_remote_code=True)\n",
    "quant_cfg = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    GEN_MODEL,\n",
    "    quantization_config=quant_cfg,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# Keep original settings (adjust if conflicts arise; unchanged here to meet \"keep other parts identical\" requirement)\n",
    "model = torch.compile(model, mode=\"reduce-overhead\")\n",
    "print(\">>> Model ready!\")\n",
    "\n",
    "# ===== Prompt Template =====\n",
    "def build_prompt(question, context):\n",
    "    return f\"\"\"You are an AI assistant. Based on the context, answer the question in the following format:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Final Answer:\"\"\"\n",
    "\n",
    "# ===== Dynamic Batch Inference =====\n",
    "def batch_generate_dynamic(prompts, initial_bs=8, max_bs=8):\n",
    "    results = []\n",
    "    i, bs = 0, initial_bs\n",
    "    last_safe_bs = initial_bs\n",
    "    while i < len(prompts):\n",
    "        batch = prompts[i:i+bs]\n",
    "        try:\n",
    "            inputs = tokenizer(batch, return_tensors=\"pt\", padding=True,\n",
    "                               truncation=True, max_length=2048).to(DEVICE)\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=GEN_MAXLEN,\n",
    "                min_new_tokens=256,\n",
    "                penalty_alpha=1.2,             # Keep unchanged\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            results.extend([d.strip() for d in decoded])\n",
    "            i += bs\n",
    "            if bs < max_bs:\n",
    "                last_safe_bs = bs\n",
    "                bs = min(bs * 2, max_bs)\n",
    "        except RuntimeError as e:\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                torch.cuda.empty_cache()\n",
    "                print(f\"⚠️ OOM at bs={bs}, rolling back to {last_safe_bs}\")\n",
    "                bs = max(last_safe_bs // 2, 1)\n",
    "                if bs < 1:\n",
    "                    print(\"❌ Even bs=1 failed, aborting.\")\n",
    "                    break\n",
    "            else:\n",
    "                raise\n",
    "    return results\n",
    "\n",
    "# ===== Metric Calculation =====\n",
    "def compute_metrics(gens, refs):\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "    _, _, bert_f1 = bert_score(gens, refs, lang=\"en\", model_type=\"roberta-large\", verbose=False)\n",
    "    res = []\n",
    "    for g, r, b in zip(gens, refs, bert_f1):\n",
    "        scr = scorer.score(r, g)\n",
    "        bleu = sentence_bleu([r.split()], g.split())\n",
    "        res.append({\n",
    "            \"rouge1\": round(scr[\"rouge1\"].fmeasure, 4),\n",
    "            \"rouge2\": round(scr[\"rouge2\"].fmeasure, 4),\n",
    "            \"rougeL\": round(scr[\"rougeL\"].fmeasure, 4),\n",
    "            \"bleu\": round(bleu, 4),\n",
    "            \"bertscore\": round(b.item(), 4)\n",
    "        })\n",
    "    return res\n",
    "\n",
    "# ====== Answer Cleaning ======\n",
    "def clean_answer(text):\n",
    "    text = text.strip()\n",
    "    if \"Final Answer:\" in text:\n",
    "        return text.split(\"Final Answer:\", 1)[1].strip()\n",
    "    paragraphs = re.split(r\"\\n\\s*\\n\", text)\n",
    "    cleaned_paras = []\n",
    "    for para in paragraphs:\n",
    "        first_line = para.strip().splitlines()[0] if para.strip() else \"\"\n",
    "        if first_line.startswith((\"Question:\", \"Context:\", \"RULES:\", \"Answer the question\")):\n",
    "            continue\n",
    "        cleaned_paras.append(para.strip())\n",
    "    return \"\\n\\n\".join(p for p in cleaned_paras if p)\n",
    "\n",
    "# ===== Top-K Configuration =====\n",
    "CTX_TOPK_LIST = [1, 2, 3, 4, 5]\n",
    "topk_list = [(k, f\"top{k}\") for k in CTX_TOPK_LIST]\n",
    "MAX_REQUIRED_K = max(CTX_TOPK_LIST)  # 5\n",
    "\n",
    "# ====== Sharding (Key Addition): Assign context_id to this shard ======\n",
    "# Only modify these lines, keep others unchanged\n",
    "NUM_SUBPARTS = 4         # Fixed: split into half\n",
    "SUBPART = 2              # Changed to 2 as per request\n",
    "\n",
    "all_groups = list(df.groupby(\"context_id\"))\n",
    "shard_groups = [g for i, g in enumerate(all_groups) if i % NUM_SHARDS == SHARD]\n",
    "sub_groups = [g for j, g in enumerate(shard_groups) if j % NUM_SUBPARTS == SUBPART]\n",
    "\n",
    "print(f\">>> Shard {SHARD}/{NUM_SHARDS} groups = {len(shard_groups)} | \"\n",
    "      f\"Subpart {SUBPART}/{NUM_SUBPARTS} = {len(sub_groups)}\")\n",
    "\n",
    "# ===== Main Loop (BM25-only, Remove ColBERT Reranking, Keep Others Unchanged) =====\n",
    "results = []\n",
    "\n",
    "for cid, grp in tqdm(sub_groups, total=len(sub_groups)):\n",
    "    batch_prompts, meta = [], []\n",
    "    for _, row in grp.iterrows():\n",
    "        q, gt, qid = row[\"question\"], row[\"answer\"], str(row[\"qa_id\"])\n",
    "\n",
    "        if eligible is not None and qid not in eligible:\n",
    "            continue\n",
    "\n",
    "        # --- BM25 with cid filtering ---\n",
    "        bm25_in = pd.DataFrame({\"qid\": [\"0\"], \"query\": [clean_query(q)]})\n",
    "        out = bm25.transform(bm25_in)\n",
    "        out = out[out[\"docno\"].str.startswith(cid)].head(BM25_TOPK)\n",
    "        ids = [int(docno_to_docid[d]) for d in out[\"docno\"] if d in docno_to_docid]\n",
    "\n",
    "        # Uniform sample requirement (ensure top5 available)\n",
    "        if eligible is None and len(ids) < MAX_REQUIRED_K:\n",
    "            continue\n",
    "\n",
    "        paras = [para_text_map[str(i)] for i in ids]\n",
    "\n",
    "        # ===== Remove ColBERT reranking, use BM25 order directly =====\n",
    "        pairs = list(zip(ids, paras, [None]*len(paras)))\n",
    "\n",
    "        if len(pairs) < MAX_REQUIRED_K:\n",
    "            continue\n",
    "\n",
    "        # --- Slice BM25 list, record topk \"rank1/2/..\" text ---\n",
    "        for topk, tag in topk_list:\n",
    "            _, top_ps, _ = zip(*pairs[:topk])\n",
    "            topk_ranked_context = \"\\n\".join([f\"rank{i+1}: {top_ps[i]}\" for i in range(len(top_ps))])\n",
    "\n",
    "            numbered = [f\"Paragraph {i+1}: {p}\" for i, p in enumerate(top_ps)]\n",
    "\n",
    "            # Run all topk sequentially; add reversed for topk>=2; add shuffled for topk>=3\n",
    "            strategies = [(\"sequential\", numbered)]\n",
    "            if topk >= 2 and len(numbered) > 1:\n",
    "                strategies += [(\"reversed\", list(reversed(numbered)))]\n",
    "            if topk >= 3 and len(numbered) > 1:\n",
    "                strategies += [(\"shuffled\", random.sample(numbered, len(numbered)))]\n",
    "\n",
    "            for strat_name, context in strategies:\n",
    "                batch_prompts.append(build_prompt(q, \"\\n\".join(context)))\n",
    "                meta.append((f\"{strat_name}_{tag}\", gt, row[\"qa_id\"], q, topk, topk_ranked_context, cid))\n",
    "\n",
    "    if not batch_prompts:\n",
    "        continue\n",
    "\n",
    "    answers = batch_generate_dynamic(batch_prompts, initial_bs=8, max_bs=8)\n",
    "    for ans, (strat, gt, qid, q, topk_val, topk_ranked_ctx, cid_val) in zip(answers, meta):\n",
    "        if not ans:\n",
    "            continue\n",
    "        ans_clean = clean_answer(ans)\n",
    "        m = compute_metrics([ans_clean], [gt])[0]\n",
    "        results.append({\n",
    "            \"cid\": cid_val,\n",
    "            \"qid\": qid,\n",
    "            \"question\": q,\n",
    "            \"topk\": topk_val,\n",
    "            \"topk_ranked_context\": topk_ranked_ctx,  # Keep column name unchanged\n",
    "            \"strategy\": strat,\n",
    "            \"answer_clean\": ans.strip(),\n",
    "            \"answer_for_eval\": ans_clean,\n",
    "            **m\n",
    "        })\n",
    "\n",
    "# ===== Save Results (With Sharding Suffix; Preserve Auto Line Breaks) =====\n",
    "df_res = pd.DataFrame(results)\n",
    "\n",
    "# Fixed column order\n",
    "cols_order = [\n",
    "    \"cid\", \"qid\", \"question\",\n",
    "    \"topk\", \"topk_ranked_context\", \"strategy\",\n",
    "    \"answer_clean\", \"answer_for_eval\",\n",
    "    \"rouge1\", \"rouge2\", \"rougeL\", \"bleu\", \"bertscore\"\n",
    "]\n",
    "df_res = df_res[cols_order]\n",
    "\n",
    "# Convert \\n in topk_ranked_context to actual newlines (auto-wrap in Excel/tables)\n",
    "df_res[\"topk_ranked_context\"] = df_res[\"topk_ranked_context\"].apply(lambda x: x.replace(\"\\n\", \"\\r\\n\"))\n",
    "\n",
    "suf = f\"_shard{SHARD}of{NUM_SHARDS}_part{SUBPART}of{NUM_SUBPARTS}\"\n",
    "df_res.to_csv(f\"{ROOT}/final_results_stage1_bm25only{suf}.csv\", index=False)\n",
    "avg_m = df_res.groupby(\"strategy\")[[\"rouge1\", \"rouge2\", \"rougeL\", \"bleu\", \"bertscore\"]].mean().reset_index()\n",
    "avg_m.to_csv(f\"{ROOT}/average_metrics_stage1_bm25only{suf}.csv\", index=False)\n",
    "\n",
    "print(\"✅ Done. Saved:\")\n",
    "print(\"   -\", f\"{ROOT}/final_results_stage1_bm25only{suf}.csv\")\n",
    "print(\"   -\", f\"{ROOT}/average_metrics_stage1_bm25only{suf}.csv\")"
   ],
   "id": "b8f1fca995e091f7"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35101989-576e-4eae-a62f-bd7a5ec527ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded final_results_stage1_bm25only_shard1of2_part3of4.csv: 408 rows\n",
      "Loaded final_results_stage1_bm25only_shard1of2_part1of4.csv: 324 rows\n",
      "Loaded final_results_stage1_bm25only_shard1of2_part0of2.csv: 768 rows\n",
      "Loaded final_results_stage1_bm25only_shard0of2.csv: 1296 rows\n",
      "Total merged rows: 2796\n",
      "✅ Merged and sorted file saved to: /gz-data/nlquad_colbert/final_results_stage1_bm25only_merged.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "ROOT = \"/gz-data/nlquad_colbert\"\n",
    "\n",
    "# 需要合并的 final_results CSV\n",
    "final_files = [\n",
    "    \"final_results_stage1_bm25only_shard1of2_part3of4.csv\",\n",
    "    \"final_results_stage1_bm25only_shard1of2_part1of4.csv\",\n",
    "    \"final_results_stage1_bm25only_shard1of2_part0of2.csv\",\n",
    "    \"final_results_stage1_bm25only_shard0of2.csv\"\n",
    "]\n",
    "\n",
    "# 输出文件名\n",
    "output_file = \"final_results_stage1_bm25only_merged.csv\"\n",
    "\n",
    "# 读取 & 合并\n",
    "dfs = []\n",
    "for fname in final_files:\n",
    "    fpath = os.path.join(ROOT, fname)\n",
    "    if os.path.exists(fpath):\n",
    "        df = pd.read_csv(fpath)\n",
    "        dfs.append(df)\n",
    "        print(f\"Loaded {fname}: {len(df)} rows\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"❌ File not found: {fpath}\")\n",
    "\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"Total merged rows: {len(merged_df)}\")\n",
    "\n",
    "# 检查重复\n",
    "dup_mask = merged_df.duplicated(subset=[\"qid\", \"strategy\", \"topk\"], keep=False)\n",
    "if dup_mask.any():\n",
    "    dup_rows = merged_df.loc[dup_mask, [\"qid\", \"strategy\", \"topk\"]]\n",
    "    raise ValueError(f\"❌ Found duplicate rows based on qid+strategy+topk:\\n{dup_rows}\")\n",
    "\n",
    "# 排序\n",
    "merged_df = merged_df.sort_values(by=[\"qid\", \"topk\", \"strategy\"]).reset_index(drop=True)\n",
    "\n",
    "# 保存\n",
    "out_path = os.path.join(ROOT, output_file)\n",
    "merged_df.to_csv(out_path, index=False)\n",
    "print(f\"✅ Merged and sorted file saved to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392b99d1-05d9-4eb9-a697-a5f570739650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Fixed file list (in the order provided or desired)\n",
    "files = [\n",
    "    \"/gz-data/nlquad_colbert/average_metrics_stage1_bm25only_shard1of2_part3of4.csv\",\n",
    "    \"/gz-data/nlquad_colbert/average_metrics_stage1_bm25only_shard1of2_part1of4.csv\",\n",
    "    \"/gz-data/nlquad_colbert/average_metrics_stage1_bm25only_shard1of2_part0of4.csv\",\n",
    "    \"/gz-data/nlquad_colbert/average_metrics_stage1_bm25only_shard0of2.csv\",\n",
    "]\n",
    "\n",
    "# 1) Check if all files exist\n",
    "missing = [f for f in files if not os.path.exists(f)]\n",
    "if missing:\n",
    "    raise FileNotFoundError(\"The following files were not found:\\n\" + \"\\n\".join(missing))\n",
    "\n",
    "# 2) Read files\n",
    "dfs = [pd.read_csv(f) for f in files]\n",
    "\n",
    "# 3) Check column consistency\n",
    "cols0 = dfs[0].columns.tolist()\n",
    "for f, df in zip(files, dfs):\n",
    "    if df.columns.tolist() != cols0:\n",
    "        raise ValueError(f\"Inconsistent columns in file: {f}\\nExpected: {cols0}\\nFound: {df.columns.tolist()}\")\n",
    "\n",
    "# 4) Merge\n",
    "merged = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# 5) Sort (by 'strategy' column if it exists)\n",
    "if \"strategy\" in merged.columns:\n",
    "    merged = merged.sort_values(by=[\"strategy\"]).reset_index(drop=True)\n",
    "\n",
    "# 6) Save, keeping four decimal places\n",
    "out_path = \"/gz-data/nlquad_colbert/average_metrics_stage1_bm25only_merged.csv\"\n",
    "merged.to_csv(out_path, index=False, float_format=\"%.4f\")\n",
    "\n",
    "print(\"✅ Merge complete, output:\", out_path)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ========== Stage 2: LLM-based Scoring (Coherence / Informativeness) ==========\n",
    "import os, re, gc, torch, warnings\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ====== Path Configuration (Kaggle-friendly) ======\n",
    "# Place the merged Stage 1 output file in Kaggle's /kaggle/input/<dataset>/ or /kaggle/working/\n",
    "CANDIDATES = [\"/kaggle/input/final-results-stage1-bm25only-merged/final_results_stage1_bm25only_merged.csv\"]  # If added as a Kaggle Dataset\n",
    "RESULT_FILE = next((p for p in CANDIDATES if os.path.exists(p)), None)\n",
    "assert RESULT_FILE is not None, f\"Stage 1 merged results file not found. Please check paths:\\n{CANDIDATES}\"\n",
    "\n",
    "# Stage 2 output to working directory\n",
    "STAGE2_FILE = \"/kaggle/working/final_results_stage2_bm25only_merged_scored.csv\"\n",
    "\n",
    "# Qwen scoring model path/ID:\n",
    "# - If mounted as a Kaggle Dataset: e.g., /kaggle/input/qwen25-7b-instruct\n",
    "# - Or, if internet is enabled, use HF Hub name (e.g., 'Qwen/Qwen2.5-7B-Instruct')\n",
    "LLM_SCORE_MODEL = \"Qwen/Qwen2.5-7B-Instruct\"  # Adjust to your actual path/ID\n",
    "\n",
    "# ====== Load NLQuAD Gold Answers ======\n",
    "print(\">>> Loading NLQuAD to recover gold answers ...\")\n",
    "dataset = load_dataset(\"LLukas22/NLQuAD\", split=\"test\")\n",
    "gold_map = {}\n",
    "for article in dataset:\n",
    "    for para in article[\"paragraphs\"]:\n",
    "        for qa in para[\"qas\"]:\n",
    "            if qa[\"answers\"]:\n",
    "                gold_map[qa[\"id\"]] = qa[\"answers\"][0][\"text\"]\n",
    "\n",
    "# ====== Load Stage 1 Merged Results (BM25-only merged) ======\n",
    "print(f\">>> Reading Stage 1 merged results from: {RESULT_FILE}\")\n",
    "df_res = pd.read_csv(RESULT_FILE)\n",
    "if \"qid\" not in df_res.columns or \"question\" not in df_res.columns or \"answer_for_eval\" not in df_res.columns:\n",
    "    raise ValueError(\"Input file missing required columns: ['qid', 'question', 'answer_for_eval']\")\n",
    "\n",
    "df_res[\"gold_answer\"] = df_res[\"qid\"].map(gold_map)\n",
    "\n",
    "# ====== Load Qwen Scoring Model (Kaggle: Single GPU Preferred) ======\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\">>> Loading scorer ({LLM_SCORE_MODEL}) on {device} ...\")\n",
    "tok_score = AutoTokenizer.from_pretrained(LLM_SCORE_MODEL, trust_remote_code=True)\n",
    "# Kaggle typically has one GPU, so device_map='auto' is sufficient; no need for balanced/max_memory\n",
    "score_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_SCORE_MODEL,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\">>> Scorer ready!\")\n",
    "\n",
    "# ====== Build Prompt ======\n",
    "def build_score_prompt(question, gold, generated):\n",
    "    return f\"\"\"\n",
    "You are an expert evaluator. Based on the question, gold answer, and generated answer, provide two scores:\n",
    "1. Coherence (1-5): Logical clarity and fluency of the generated answer.\n",
    "2. Informativeness (1-5): Amount of correct and relevant information compared to the gold answer.\n",
    "\n",
    "Question: {question}\n",
    "Gold Answer: {gold}\n",
    "Generated Answer: {generated}\n",
    "\n",
    "IMPORTANT:\n",
    "- Do NOT provide any explanation or extra text.\n",
    "- Only output two integers, separated by a comma, in the LAST line.\n",
    "Format:\n",
    "x,y\n",
    "\n",
    "Now output the result:\n",
    "\"\"\"\n",
    "\n",
    "# ====== Parse LLM Output ======\n",
    "def parse_llm_output(text):\n",
    "    lines = text.strip().splitlines()\n",
    "    for line in reversed(lines):\n",
    "        if re.match(r\"^\\s*[1-5]\\s*,\\s*[1-5]\\s*$\", line):\n",
    "            a, b = [int(x.strip()) for x in line.split(\",\")]\n",
    "            return {\"coherence\": a, \"informativeness\": b}\n",
    "    nums = re.findall(r\"\\b[1-5]\\b\", text)\n",
    "    if len(nums) >= 2:\n",
    "        return {\"coherence\": int(nums[-2]), \"informativeness\": int(nums[-1])}\n",
    "    return {\"coherence\": 0, \"informativeness\": 0}\n",
    "\n",
    "# ====== Batch Generate Scores (With Fallback for OOM) ======\n",
    "def batch_generate_scores(prompts, initial_bs=2, min_bs=1, max_new_tokens=8):\n",
    "    results = []\n",
    "    i, bs = 0, initial_bs\n",
    "    while i < len(prompts):\n",
    "        batch = prompts[i:i+bs]\n",
    "        try:\n",
    "            inputs = tok_score(batch, return_tensors=\"pt\", padding=True, truncation=True).to(score_model.device)\n",
    "            with torch.inference_mode():\n",
    "                outputs = score_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=tok_score.eos_token_id\n",
    "                )\n",
    "            decoded = tok_score.batch_decode(outputs, skip_special_tokens=True)\n",
    "            results.extend(decoded)\n",
    "            i += bs\n",
    "            if bs < initial_bs:\n",
    "                bs = min(bs * 2, initial_bs)\n",
    "        except RuntimeError as e:\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                torch.cuda.empty_cache()\n",
    "                new_bs = max(bs // 2, min_bs)\n",
    "                print(f\"⚠️ OOM at batch_size={bs}, reducing to {new_bs}\")\n",
    "                bs = new_bs\n",
    "                if bs < 1:\n",
    "                    raise RuntimeError(\"❌ Even batch_size=1 failed. Aborting Stage 2.\")\n",
    "            else:\n",
    "                raise\n",
    "    return results\n",
    "\n",
    "# ====== Stage 2 Main Loop ======\n",
    "print(\">>> Stage 2 (LLM-based scoring) running ...\")\n",
    "BATCH_SIZE = 2 if torch.cuda.is_available() else 2   # Kaggle GPU: 8 is stable; smaller for CPU\n",
    "prompts, meta, results_s2 = [], [], []\n",
    "\n",
    "for _, row in tqdm(df_res.iterrows(), total=len(df_res)):\n",
    "    prompt = build_score_prompt(row[\"question\"], row[\"gold_answer\"], row[\"answer_for_eval\"])\n",
    "    prompts.append(prompt)\n",
    "    meta.append(row.to_dict())\n",
    "\n",
    "    if len(prompts) >= BATCH_SIZE:\n",
    "        outs = batch_generate_scores(prompts, initial_bs=BATCH_SIZE)\n",
    "        for out, row_data in zip(outs, meta):\n",
    "            sc = parse_llm_output(out)\n",
    "            row_data.update(sc)\n",
    "            results_s2.append(row_data)\n",
    "        prompts, meta = [], []\n",
    "\n",
    "# Flush remaining\n",
    "if prompts:\n",
    "    outs = batch_generate_scores(prompts, initial_bs=BATCH_SIZE)\n",
    "    for out, row_data in zip(outs, meta):\n",
    "        sc = parse_llm_output(out)\n",
    "        row_data.update(sc)\n",
    "        results_s2.append(row_data)\n",
    "\n",
    "# ====== Save Stage 2 Results ======\n",
    "out_df = pd.DataFrame(results_s2)\n",
    "out_df.to_csv(STAGE2_FILE, index=False)\n",
    "print(f\"✅ Stage 2 completed. Results saved at {STAGE2_FILE}\")"
   ],
   "id": "d0b42292a0a503a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ========== Stage 2: LLM-based Scoring (Coherence / Informativeness) ==========\n",
    "import os, re, gc, torch, warnings\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ====== Path Configuration (Kaggle-friendly) ======\n",
    "# Place your Stage 1 merged output file in Kaggle's /kaggle/input/<dataset>/ or /kaggle/working/\n",
    "CANDIDATES = [\"/kaggle/input/final-results-stage1-bm25only-merged/final_results_stage1_bm25only_merged.csv\"]  # If added as a Kaggle Dataset\n",
    "RESULT_FILE = next((p for p in CANDIDATES if os.path.exists(p)), None)\n",
    "assert RESULT_FILE is not None, f\"Stage 1 merged results file not found. Check paths:\\n{CANDIDATES}\"\n",
    "\n",
    "# Stage 2 output to working directory\n",
    "STAGE2_FILE = \"/kaggle/working/final_results_stage2_bm25only_merged_scored.csv\"\n",
    "\n",
    "# Qwen scoring model path/ID:\n",
    "# - If mounted as a Kaggle Dataset: e.g., /kaggle/input/qwen25-7b-instruct\n",
    "# - Or if internet is enabled, use HF Hub name (e.g., 'Qwen/Qwen2.5-7B-Instruct')\n",
    "LLM_SCORE_MODEL = \"Qwen/Qwen2.5-7B-Instruct\"  # Adjust to your actual path/ID\n",
    "\n",
    "# ====== Load NLQuAD Gold Answers ======\n",
    "print(\">>> Loading NLQuAD to recover gold answers ...\")\n",
    "dataset = load_dataset(\"LLukas22/NLQuAD\", split=\"test\")\n",
    "gold_map = {}\n",
    "for article in dataset:\n",
    "    for para in article[\"paragraphs\"]:\n",
    "        for qa in para[\"qas\"]:\n",
    "            if qa[\"answers\"]:\n",
    "                gold_map[qa[\"id\"]] = qa[\"answers\"][0][\"text\"]\n",
    "\n",
    "# ====== Load Stage 1 Merged Results (BM25-only merged) ======\n",
    "print(f\">>> Reading Stage 1 merged results from: {RESULT_FILE}\")\n",
    "df_res = pd.read_csv(RESULT_FILE)\n",
    "if \"qid\" not in df_res.columns or \"question\" not in df_res.columns or \"answer_for_eval\" not in df_res.columns:\n",
    "    raise ValueError(\"Input file lacks required columns: ['qid','question','answer_for_eval']\")\n",
    "\n",
    "df_res[\"gold_answer\"] = df_res[\"qid\"].map(gold_map)\n",
    "\n",
    "# ====== Load Qwen Scoring Model (Kaggle: Single GPU preferred) ======\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\">>> Loading scorer ({LLM_SCORE_MODEL}) on {device} ...\")\n",
    "tok_score = AutoTokenizer.from_pretrained(LLM_SCORE_MODEL, trust_remote_code=True)\n",
    "# Kaggle typically has one GPU, so device_map='auto' is sufficient; no need for balanced/max_memory\n",
    "score_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_SCORE_MODEL,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\">>> Scorer ready!\")\n",
    "\n",
    "# ====== Construct Prompt ======\n",
    "def build_score_prompt(question, gold, generated):\n",
    "    return f\"\"\"\n",
    "You are an expert evaluator. Based on the question, gold answer, and generated answer, provide two scores:\n",
    "1. Coherence (1-5): Logical clarity and fluency of the generated answer.\n",
    "2. Informativeness (1-5): Amount of correct and relevant information compared to the gold answer.\n",
    "\n",
    "Question: {question}\n",
    "Gold Answer: {gold}\n",
    "Generated Answer: {generated}\n",
    "\n",
    "IMPORTANT:\n",
    "- Do NOT provide any explanation or extra text.\n",
    "- Only output two integers, separated by a comma, in the LAST line.\n",
    "Format:\n",
    "x,y\n",
    "\n",
    "Now output the result:\n",
    "\"\"\"\n",
    "\n",
    "# ====== Parse LLM Output ======\n",
    "def parse_llm_output(text):\n",
    "    lines = text.strip().splitlines()\n",
    "    for line in reversed(lines):\n",
    "        if re.match(r\"^\\s*[1-5]\\s*,\\s*[1-5]\\s*$\", line):\n",
    "            a, b = [int(x.strip()) for x in line.split(\",\")]\n",
    "            return {\"coherence\": a, \"informativeness\": b}\n",
    "    nums = re.findall(r\"\\b[1-5]\\b\", text)\n",
    "    if len(nums) >= 2:\n",
    "        return {\"coherence\": int(nums[-2]), \"informativeness\": int(nums[-1])}\n",
    "    return {\"coherence\": 0, \"informativeness\": 0}\n",
    "\n",
    "# ====== Batch Scoring (with Batch Size Fallback to Prevent OOM) ======\n",
    "def batch_generate_scores(prompts, initial_bs=2, min_bs=1, max_new_tokens=8):\n",
    "    results = []\n",
    "    i, bs = 0, initial_bs\n",
    "    while i < len(prompts):\n",
    "        batch = prompts[i:i+bs]\n",
    "        try:\n",
    "            inputs = tok_score(batch, return_tensors=\"pt\", padding=True, truncation=True).to(score_model.device)\n",
    "            with torch.inference_mode():\n",
    "                outputs = score_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=tok_score.eos_token_id\n",
    "                )\n",
    "            decoded = tok_score.batch_decode(outputs, skip_special_tokens=True)\n",
    "            results.extend(decoded)\n",
    "            i += bs\n",
    "            if bs < initial_bs:\n",
    "                bs = min(bs * 2, initial_bs)\n",
    "        except RuntimeError as e:\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                torch.cuda.empty_cache()\n",
    "                new_bs = max(bs // 2, min_bs)\n",
    "                print(f\"⚠️ OOM at batch_size={bs}, reducing to {new_bs}\")\n",
    "                bs = new_bs\n",
    "                if bs < 1:\n",
    "                    raise RuntimeError(\"❌ Even batch_size=1 failed. Aborting Stage 2.\")\n",
    "            else:\n",
    "                raise\n",
    "    return results\n",
    "\n",
    "# ====== Stage 2 Main Loop ======\n",
    "print(\">>> Stage 2 (LLM-based scoring) running ...\")\n",
    "BATCH_SIZE = 2 if torch.cuda.is_available() else 2   # Kaggle GPU: 8 is stable; CPU: smaller batch\n",
    "prompts, meta, results_s2 = [], [], []\n",
    "\n",
    "for _, row in tqdm(df_res.iterrows(), total=len(df_res)):\n",
    "    prompt = build_score_prompt(row[\"question\"], row[\"gold_answer\"], row[\"answer_for_eval\"])\n",
    "    prompts.append(prompt)\n",
    "    meta.append(row.to_dict())\n",
    "\n",
    "    if len(prompts) >= BATCH_SIZE:\n",
    "        outs = batch_generate_scores(prompts, initial_bs=BATCH_SIZE)\n",
    "        for out, row_data in zip(outs, meta):\n",
    "            sc = parse_llm_output(out)\n",
    "            row_data.update(sc)\n",
    "            results_s2.append(row_data)\n",
    "        prompts, meta = [], []\n",
    "\n",
    "# Flush remaining\n",
    "if prompts:\n",
    "    outs = batch_generate_scores(prompts, initial_bs=BATCH_SIZE)\n",
    "    for out, row_data in zip(outs, meta):\n",
    "        sc = parse_llm_output(out)\n",
    "        row_data.update(sc)\n",
    "        results_s2.append(row_data)\n",
    "\n",
    "# ====== Save Stage 2 Results ======\n",
    "out_df = pd.DataFrame(results_s2)\n",
    "out_df.to_csv(STAGE2_FILE, index=False)\n",
    "print(f\"✅ Stage 2 completed. Results saved at {STAGE2_FILE}\")"
   ],
   "id": "cf84aaa09774940e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv @gz-data)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
